---
title: "abundance"
author: "Carl Boettiger"
date: "2020-07-21"
output: 
  github_document:
    df_print: tibble
---


```{r setup, message=F}
library(dplyr)
```


```{r}
## or just use a cache committed to the repo, since it's small
bet_sorting <- readRDS("data/bet_sorting.rds")
bet_fielddata <- readRDS("data/bet_fielddata.rds")
```


## Compute 'catch-per-unit-effort', CPUE, by date and site

```{r message = FALSE}
effort <- bet_fielddata %>% 
  group_by(siteID, collectDate) %>% 
  summarize(trapnights = as.integer(sum(collectDate - setDate)))
  #summarize(trapnights = sum(trappingDays))  ## Has bunch of NAs this way

counts <- bet_sorting %>%  
## Are we really aggregating all beetles regardless of species ID?  Do we want to do top 10 most abund species instead?
#  group_by(scientificName, collectDate, siteID) %>%
  group_by(collectDate, siteID) %>%
    summarize(count = sum(individualCount, na.rm = TRUE))

bet_cpue_raw <- counts %>% 
  left_join(effort) %>% 
  mutate(cpue = count / trapnights) %>% ungroup()

# damn it's takes a lot of trap nights to catch any beetles!
bet_cpue_raw
```


Let's use the 2019 data as the "future" to assess forecast skill.  Also, let's aggregate by month so it's a bit easier to line up 'historic' and 'predicted' values over a seasonal trend.  

```{r  message=F}
bet_cpue <- bet_cpue_raw %>% 
  mutate(month = format(collectDate, "%Y-%m")) %>%
  mutate(month = as.Date(paste(month, "01", sep="-")))

future_cpue <- bet_cpue %>% filter(collectDate >= "2019-01-01")
train_cpue <- bet_cpue %>% filter(collectDate < "2019-01-01")
```

## Average baseline dummy forecast  

Note: we average all historical months to forecast the future month, so that our dummy forecast can still reflect seasonal variability to some extent.  Note also that for each month of a given year we usually have two bouts of sampling, so those are also just being averaged in here.  

```{r  message=F}
null_forecast <- train_cpue %>% 
  mutate(month = lubridate::month(month, label=TRUE)) %>%
  group_by(month, siteID) %>%
  summarize(mu = mean(cpue, na.rm=TRUE),
            sigma = sd(cpue, na.rm=TRUE))

null_forecast
```


## Scoring

We can easily compute a proper score for each site:


```{r  message=F}

proper_score <- function(x, mu, sigma){ -(mu - x )^2 / sigma^2  - log(sigma) }


proper_scores <- future_cpue %>%
  mutate(month = lubridate::month(month, label=TRUE)) %>%
  select(month, siteID, true = cpue) %>% 
  left_join(null_forecast)  %>%
  mutate(score = proper_score(true, mu, sigma))
```


We could also compute a net score by site (summed over month) or overall score (at least I think you're allowed to just sum proper scores):

```{r}
proper_scores %>%  group_by(siteID) %>% summarize(sum(score, na.rm = TRUE))
proper_scores %>% pull(score) %>% sum(na.rm = TRUE)
```
