---
title: "abundance"
author: "Anna Spiers"
date: "7/9/2020"
output: github_document
---


```{r setup, message=FALSE}
library(tidyverse)
Sys.setenv("NEONSTORE_HOME" = "cache/")

## Set the year of the prediction.  This will be excluded from the training data and used in the forecast
forecast_year <- 2019
```

# Download data

```{r}
## full API queries take ~ 2m
## full download takes ~ 5m on Jetstream
start_date <- NA # Update to most recent download, or NA to get all
neonstore::neon_download(product="DP1.10022.001", start_date = start_date)
```

## Load data

```{r message=FALSE}
library(neonstore)
sorting <- neon_read("bet_sorting", altrep = FALSE)
para <- neon_read("bet_parataxonomistID", altrep = FALSE)
expert <- neon_read("bet_expertTaxonomistIDProcessed", altrep = FALSE)
field <- neon_read("bet_fielddata", altrep = FALSE)
```

## Process data

Resolve taxonomy using the expert and parataxonomist classification where available.  

For convenience, we also add month and year as separate columns from the collectDate, allowing for easy grouping.

```{r}
source("R/taxonomy.R")
beetles <- resolve_taxonomy(sorting, para, expert) %>% 
  mutate(month = lubridate::month(collectDate, label=TRUE),
         year =  lubridate::year(collectDate))

```



## Generate derived richness product

Focuses on `taxonID` as the unit of taxonomy, which corresponds to best resolved scientific name.  Use `morphospecies` to focus on species-level (binomal names only),
using morphospecies where available and where official classification was only resolved to a higher rank. 

```{r}
richness <- beetles %>%  
  select(taxonID, siteID, collectDate, month, year) %>%
  distinct() %>%
  count(siteID, collectDate, month, year)
```

## Generate derived CPUE product

We target a catch-per-unit-effort (CPUE) metric for abundance, e.g. to avoid the problem of having contestants have to predict the number of trap nights there will be.  (Quite a warranted concern for 2020!  Overall variability is over 30%, while 2018 & 2019 it is 22%.)  This does suggest the assumption that trapnights are exchangable, but teams accounting for things like weather on each night could still forecast raw counts and then convert their forecast to this simpler metric.  


```{r}
effort <- field %>% 
  group_by(siteID, collectDate) %>% 
  summarize(trapnights = as.integer(sum(collectDate - setDate)))
  #summarize(trapnights = sum(trappingDays))  ## Has bunch of NAs this way

counts <- beetles %>%  
#  group_by(scientificName, collectDate, siteID) %>%
  group_by(collectDate, siteID, year, month) %>%
    summarize(count = sum(individualCount, na.rm = TRUE))

cpue_df <- counts %>% 
  left_join(effort) %>% 
  mutate(cpue = count / trapnights) %>% ungroup()


cpue_df 
```

## Register (publish) derived products



```{r}

```




## Compute (null model) Forecasts

### Baseline forecast  

For the groups with only 1 data point we cannot compute `sd`, let's use the average `sd` of all the other data instead as our guess.
Note that some months may wind up having caught beetles in the future, even though we have no catch in the data to date.  These will
end up as `NA` scores unless we include a mechanism to convert them to estimates (e.g. we should probably estimate a value of 0 for all months
for which we have no catch.)


To mimic scoring our forecast, we will remove data from `r forecast_year` or later.  The actual null forecast should of course omit that filter.  

```{r}
null_richness <- richness %>% 
    filter(year < forecast_year) %>%
  group_by(month, siteID) %>%
  summarize(mean = mean(n, na.rm = TRUE),
            sd = sd(n, na.rm = TRUE)) %>% 
  mutate(sd = replace_na(sd, mean(sd, na.rm=TRUE))) %>% 
  mutate(year = forecast_year)

null_richness
```


```{r}
null_cpue <- cpue_df %>% 
      filter(year < forecast_year) %>%
  group_by(month, siteID) %>%
  summarize(mean = mean(cpue, na.rm=TRUE),
            sd = sd(cpue, na.rm=TRUE))  %>% 
  mutate(sd = replace_na(sd, mean(sd, na.rm=TRUE))) %>% 
  mutate(year = forecast_year)

```


### Register (publish) the forecast




## Score the forecast


```{r}
## predicted_df must have columns: mean, sd, and any grouping variables (siteID, month)
## true_df must have column: 'true' and the same grouping variables with same colname and datatype
score <- function(predicted_df,
                  true_df,
                  scoring_fn =  function(x, mu, sigma){ -(mu - x )^2 / sigma^2  - log(sigma)}
                  ){
  true_df %>% 
  left_join(predicted_df)  %>%
  mutate(score = scoring_fn(true, mean, sd))
}

```  


Extract the true richnesses for `r forecast_year` and compute score:

```{r message=FALSE}
true_richness <- richness %>%
  filter(year >= forecast_year) %>%
  select(month, siteID, true = n)

richness_score <- score(null_richness, true_richness)
```

Extract the true CPUE  for `r forecast_year` and compute score


```{r message=FALSE}
true_cpue <- cpue_df %>%
  filter(year >= forecast_year) %>%
  select(month, siteID, true = cpue)

cpue_score <- score(null_cpue, true_cpue)
```




```{r}
readr::write_csv(score_df, "richness_score.csv")
```


Note that removing `NA`s in a sum of scores is unfair, as "0" reflects a perfect score.  
To avoid this, one option is to compute the mean score across sites:

```{r}
richness_score %>% summarize(mean_score = mean(score, na.rm= TRUE))
cpue_score %>% summarize(mean_score = mean(score, na.rm= TRUE))

```













